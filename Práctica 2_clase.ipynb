{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPakZPTRfn1ScSraZHG2jNs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rosalba23/Repositorio1/blob/master/Pr%C3%A1ctica%202_clase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RrJ_VL5cibD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E5K4XBHcq_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6c2f3420-e2e2-4f70-cc15-114d343e0b85"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RBPtjnQcy31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import numpy as np    \n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ_BFuVwc3jd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "cb110fc3-19c6-492b-8102-85ee123c7c86"
      },
      "source": [
        "stopwords.fileids()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arabic',\n",
              " 'azerbaijani',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'turkish']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBk_R6zQdPgh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d88f321-f152-4f92-cec7-05f2a9f82030"
      },
      "source": [
        "set(stopwords.words('english'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DomDmvFBdV-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYuT7lwIf-zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_01 = \"I have been very passionate about automating machine learning myself ever since our Automatic Statistician project started back in 2014. I want us to be really ambitious in this endeavor; we should try to automate all aspects of the entire machine learning and data analysis pipeline. This includes automating data collection and experiment design; automating data cleanup and missing data imputa- tion; automating feature selection and transformation; automating model discovery, criticism, and explanation; automating the allocation of computational resources; automating hyperparameter optimization; automating inference; and automating model monitoring and anomaly detection. This is a huge list of things, and we’d optimally like to automate all of it.\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waS6hjgGgI_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_02 = \"There is a caveat of course. While full automation can motivate scientific research and provide a long-term engineering goal, in practice, we probably want to semiautomate most of these and gradually remove the human in the loop as needed. Along the way, what is going to happen if we try to do all this automation is that we are likely to develop powerful tools that will help make the practice of machine learning, first of all, more systematic (since it’s very ad hoc these days) and also more efficient.\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98aINhGRgLnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_03 = \"These are worthy goals even if we did not succeed in the final goal of automation, but as this book demonstrates, current AutoML methods can already surpass human machine learning experts in several tasks. This trend is likely only going to intensify as we’re making progress and as computation becomes ever cheaper, and AutoML is therefore clearly one of the topics that is here to stay. It is a great time to get involved in AutoML, and this book is an excellent starting point.\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_849DoFwgefK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_01 = \"Quisiera entender que le ven de rico o divertido a hacer cebo en el metro, de verdad, acabo de ver a cuatro parejas en la misma área dandolo todo con su queso\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Db8c0bcgf5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_02 = \"Dos mujeres que se tomaban una selfie al lado de una pista de aterrizaje serrana murieron, al parecer, accidentalmente; un piloto aviador que solía viajar a la Sierra Tarahumara y un maestro de artes marciales retirado fueron asesinados en 2017.\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DU74lE5gj4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_03 = \"Buenos días desde un país donde el Subsecretario de Educación dice que el comunismo es necesario para transformar a México; donde se disparan 312% los casos de dengue porque no compraron insecticidas, Mireles llama pirujas a las concubinas y el Presi quiere quitar el INE Café?\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDnwfDDxglRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7f9fd72f-2f58-43d2-a830-8b38869d13ab"
      },
      "source": [
        "text_01_tokens = tokenizer.tokenize(text_01.lower()) #tokenizar y quitar signos de puntuación\n",
        "#print(text_01_tokens)\n",
        "\n",
        "text_01_tokens_wout_stopwords = []\n",
        "\n",
        "for word in text_01_tokens:\n",
        "    if word not in stopwords.words('spanish'): text_01_tokens_wout_stopwords.append(word)\n",
        "\n",
        "print(text_01_tokens_wout_stopwords)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'have', 'been', 'very', 'passionate', 'about', 'automating', 'machine', 'learning', 'myself', 'ever', 'since', 'our', 'automatic', 'statistician', 'project', 'started', 'back', 'in', '2014', 'i', 'want', 'us', 'to', 'be', 'really', 'ambitious', 'in', 'this', 'endeavor', 'we', 'should', 'try', 'to', 'automate', 'all', 'aspects', 'of', 'the', 'entire', 'machine', 'learning', 'and', 'data', 'analysis', 'pipeline', 'this', 'includes', 'automating', 'data', 'collection', 'and', 'experiment', 'design', 'automating', 'data', 'cleanup', 'and', 'missing', 'data', 'imputa', 'tion', 'automating', 'feature', 'selection', 'and', 'transformation', 'automating', 'model', 'discovery', 'criticism', 'and', 'explanation', 'automating', 'the', 'allocation', 'of', 'computational', 'resources', 'automating', 'hyperparameter', 'optimization', 'automating', 'inference', 'and', 'automating', 'model', 'monitoring', 'and', 'anomaly', 'detection', 'this', 'is', 'huge', 'list', 'of', 'things', 'and', 'we', 'd', 'optimally', 'like', 'to', 'automate', 'all', 'of', 'it']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLkzWlffg8WI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a81dd32-9e94-45b0-a2fc-51033507b811"
      },
      "source": [
        "text_01_tokens = tokenizer.tokenize(text_01.lower()) #tokenizar y quitar signos de puntuación\n",
        "#print(text_01_tokens)\n",
        "\n",
        "text_01_tokens_wout_stopwords = []\n",
        "\n",
        "for word in text_01_tokens:\n",
        "    if word not in stopwords.words('english'): text_01_tokens_wout_stopwords.append(word)\n",
        "\n",
        "print(text_01_tokens_wout_stopwords)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['passionate', 'automating', 'machine', 'learning', 'ever', 'since', 'automatic', 'statistician', 'project', 'started', 'back', '2014', 'want', 'us', 'really', 'ambitious', 'endeavor', 'try', 'automate', 'aspects', 'entire', 'machine', 'learning', 'data', 'analysis', 'pipeline', 'includes', 'automating', 'data', 'collection', 'experiment', 'design', 'automating', 'data', 'cleanup', 'missing', 'data', 'imputa', 'tion', 'automating', 'feature', 'selection', 'transformation', 'automating', 'model', 'discovery', 'criticism', 'explanation', 'automating', 'allocation', 'computational', 'resources', 'automating', 'hyperparameter', 'optimization', 'automating', 'inference', 'automating', 'model', 'monitoring', 'anomaly', 'detection', 'huge', 'list', 'things', 'optimally', 'like', 'automate']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeKsPhb9grhF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8e950d3f-7fb6-4ab3-819f-56516751675c"
      },
      "source": [
        "text_02_tokens = tokenizer.tokenize(text_02.lower()) \n",
        "#print(text_02_tokens)\n",
        "\n",
        "text_02_tokens_wout_stopwords = []\n",
        "\n",
        "for word in text_02_tokens:\n",
        "    if word not in stopwords.words('english'): text_02_tokens_wout_stopwords.append(word)\n",
        "\n",
        "print(text_02_tokens_wout_stopwords)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dos', 'mujeres', 'que', 'se', 'tomaban', 'una', 'selfie', 'al', 'lado', 'de', 'una', 'pista', 'de', 'aterrizaje', 'serrana', 'murieron', 'al', 'parecer', 'accidentalmente', 'un', 'piloto', 'aviador', 'que', 'solía', 'viajar', 'la', 'sierra', 'tarahumara', 'un', 'maestro', 'de', 'artes', 'marciales', 'retirado', 'fueron', 'asesinados', 'en', '2017']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKMPPv4dhNkU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b9dcbc97-2f8f-4ec6-e530-e38c11cbd32a"
      },
      "source": [
        "text_03_tokens = tokenizer.tokenize(text_03.lower()) \n",
        "#print(text_03_tokens)\n",
        "\n",
        "text_03_tokens_wout_stopwords = []\n",
        "\n",
        "for word in text_03_tokens:\n",
        "    if word not in stopwords.words('spanish'): text_03_tokens_wout_stopwords.append(word)\n",
        "\n",
        "print(text_03_tokens_wout_stopwords)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['buenos', 'días', 'país', 'subsecretario', 'educación', 'dice', 'comunismo', 'necesario', 'transformar', 'méxico', 'disparan', '312', 'casos', 'dengue', 'compraron', 'insecticidas', 'mireles', 'llama', 'pirujas', 'concubinas', 'presi', 'quiere', 'quitar', 'ine', 'café']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9acMmmkhfJf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5cdef566-4dd7-4e77-8968-624a5ded8d02"
      },
      "source": [
        "print(len(text_01_tokens_wout_stopwords))\n",
        "print(len(text_02_tokens_wout_stopwords))\n",
        "print(len(text_03_tokens_wout_stopwords))\n",
        "print(len(text_01_tokens_wout_stopwords) + len(text_02_tokens_wout_stopwords) + len(text_01_tokens_wout_stopwords))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29\n",
            "38\n",
            "25\n",
            "96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9a5QL91hlrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dicc_texts = {\"text_01\": text_01_tokens_wout_stopwords, \n",
        " \"text_02\": text_02_tokens_wout_stopwords, \n",
        " \"text_03\": text_03_tokens_wout_stopwords}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgGmorTWh2BG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe215e12-07b4-48c5-fe6f-cf6d9068112d"
      },
      "source": [
        "dicc_termns = {}\n",
        "\n",
        "for text in dicc_texts:\n",
        "    for word in dicc_texts[text]:\n",
        "        \n",
        "#        print(\"EVALUAR:\", word, \"EN\", text)\n",
        "        \n",
        "        if(word in dicc_termns):#incrementar palabras al diccionario\n",
        "            dicc_termns[word] = dicc_termns[word] + 1\n",
        "            \n",
        "#            print(word, \"IN\", \"dicc_termns\")\n",
        "            \n",
        "        elif(word not in dicc_termns):#agregar palabras al diccionario        \n",
        "            dicc_termns[word] = 1\n",
        "            \n",
        "#            print(word, \"NOT IN\", \"dicc_termns\")            \n",
        "\n",
        "print(len(dicc_termns))\n",
        "dicc_termns"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2017': 1,\n",
              " '312': 1,\n",
              " 'acabo': 1,\n",
              " 'accidentalmente': 1,\n",
              " 'al': 2,\n",
              " 'artes': 1,\n",
              " 'asesinados': 1,\n",
              " 'aterrizaje': 1,\n",
              " 'aviador': 1,\n",
              " 'buenos': 1,\n",
              " 'café': 1,\n",
              " 'casos': 1,\n",
              " 'cebo': 1,\n",
              " 'compraron': 1,\n",
              " 'comunismo': 1,\n",
              " 'con': 1,\n",
              " 'concubinas': 1,\n",
              " 'cuatro': 1,\n",
              " 'dandolo': 1,\n",
              " 'de': 6,\n",
              " 'dengue': 1,\n",
              " 'dice': 1,\n",
              " 'disparan': 1,\n",
              " 'divertido': 1,\n",
              " 'dos': 1,\n",
              " 'días': 1,\n",
              " 'educación': 1,\n",
              " 'el': 1,\n",
              " 'en': 3,\n",
              " 'entender': 1,\n",
              " 'fueron': 1,\n",
              " 'hacer': 1,\n",
              " 'ine': 1,\n",
              " 'insecticidas': 1,\n",
              " 'la': 2,\n",
              " 'lado': 1,\n",
              " 'le': 1,\n",
              " 'llama': 1,\n",
              " 'maestro': 1,\n",
              " 'marciales': 1,\n",
              " 'metro': 1,\n",
              " 'mireles': 1,\n",
              " 'misma': 1,\n",
              " 'mujeres': 1,\n",
              " 'murieron': 1,\n",
              " 'méxico': 1,\n",
              " 'necesario': 1,\n",
              " 'parecer': 1,\n",
              " 'parejas': 1,\n",
              " 'país': 1,\n",
              " 'piloto': 1,\n",
              " 'pirujas': 1,\n",
              " 'pista': 1,\n",
              " 'presi': 1,\n",
              " 'que': 3,\n",
              " 'queso': 1,\n",
              " 'quiere': 1,\n",
              " 'quisiera': 1,\n",
              " 'quitar': 1,\n",
              " 'retirado': 1,\n",
              " 'rico': 1,\n",
              " 'se': 1,\n",
              " 'selfie': 1,\n",
              " 'serrana': 1,\n",
              " 'sierra': 1,\n",
              " 'solía': 1,\n",
              " 'su': 1,\n",
              " 'subsecretario': 1,\n",
              " 'tarahumara': 1,\n",
              " 'todo': 1,\n",
              " 'tomaban': 1,\n",
              " 'transformar': 1,\n",
              " 'un': 2,\n",
              " 'una': 2,\n",
              " 'ven': 1,\n",
              " 'ver': 1,\n",
              " 'verdad': 1,\n",
              " 'viajar': 1,\n",
              " 'área': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pr7Hwgqh5f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix = np.zeros((len(dicc_texts), len(dicc_termns))) # Pre-allocate matrix\n",
        "#matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgrW5Dksi6bd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c431a65-e424-41a3-a739-cb80d9f11a99"
      },
      "source": [
        "i = 0\n",
        "j = 0\n",
        "\n",
        "for word_termns in dicc_termns: #dicc_termns todos los términos\n",
        "#    print()\n",
        "    for word_texts in dicc_texts: #dicc_texts todos los textos\n",
        "#        print(\"EVALUAR:\", word_termns, \"EN: \", word_texts)\n",
        "        if(word_termns in dicc_texts[word_texts]): #si está\n",
        "            print(word_termns, \"IN\", word_texts)\n",
        "            \n",
        "            matrix[j, i] = 1\n",
        "            \n",
        "        elif(word_termns not in dicc_texts[word_texts]): # si no está\n",
        "            print(word_termns, \"NOT IN\", word_texts)\n",
        "            \n",
        "            matrix[j, i] = 0\n",
        "            \n",
        "            \n",
        "        print(\"se agregó: \", matrix[j,i], \"en: \", j, i)\n",
        "            \n",
        "        j = j + 1\n",
        "        \n",
        "    j = 0\n",
        "    i = i + 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quisiera IN text_01\n",
            "se agregó:  1.0 en:  0 0\n",
            "quisiera NOT IN text_02\n",
            "se agregó:  0.0 en:  1 0\n",
            "quisiera NOT IN text_03\n",
            "se agregó:  0.0 en:  2 0\n",
            "entender IN text_01\n",
            "se agregó:  1.0 en:  0 1\n",
            "entender NOT IN text_02\n",
            "se agregó:  0.0 en:  1 1\n",
            "entender NOT IN text_03\n",
            "se agregó:  0.0 en:  2 1\n",
            "que IN text_01\n",
            "se agregó:  1.0 en:  0 2\n",
            "que IN text_02\n",
            "se agregó:  1.0 en:  1 2\n",
            "que NOT IN text_03\n",
            "se agregó:  0.0 en:  2 2\n",
            "le IN text_01\n",
            "se agregó:  1.0 en:  0 3\n",
            "le NOT IN text_02\n",
            "se agregó:  0.0 en:  1 3\n",
            "le NOT IN text_03\n",
            "se agregó:  0.0 en:  2 3\n",
            "ven IN text_01\n",
            "se agregó:  1.0 en:  0 4\n",
            "ven NOT IN text_02\n",
            "se agregó:  0.0 en:  1 4\n",
            "ven NOT IN text_03\n",
            "se agregó:  0.0 en:  2 4\n",
            "de IN text_01\n",
            "se agregó:  1.0 en:  0 5\n",
            "de IN text_02\n",
            "se agregó:  1.0 en:  1 5\n",
            "de NOT IN text_03\n",
            "se agregó:  0.0 en:  2 5\n",
            "rico IN text_01\n",
            "se agregó:  1.0 en:  0 6\n",
            "rico NOT IN text_02\n",
            "se agregó:  0.0 en:  1 6\n",
            "rico NOT IN text_03\n",
            "se agregó:  0.0 en:  2 6\n",
            "divertido IN text_01\n",
            "se agregó:  1.0 en:  0 7\n",
            "divertido NOT IN text_02\n",
            "se agregó:  0.0 en:  1 7\n",
            "divertido NOT IN text_03\n",
            "se agregó:  0.0 en:  2 7\n",
            "hacer IN text_01\n",
            "se agregó:  1.0 en:  0 8\n",
            "hacer NOT IN text_02\n",
            "se agregó:  0.0 en:  1 8\n",
            "hacer NOT IN text_03\n",
            "se agregó:  0.0 en:  2 8\n",
            "cebo IN text_01\n",
            "se agregó:  1.0 en:  0 9\n",
            "cebo NOT IN text_02\n",
            "se agregó:  0.0 en:  1 9\n",
            "cebo NOT IN text_03\n",
            "se agregó:  0.0 en:  2 9\n",
            "en IN text_01\n",
            "se agregó:  1.0 en:  0 10\n",
            "en IN text_02\n",
            "se agregó:  1.0 en:  1 10\n",
            "en NOT IN text_03\n",
            "se agregó:  0.0 en:  2 10\n",
            "el IN text_01\n",
            "se agregó:  1.0 en:  0 11\n",
            "el NOT IN text_02\n",
            "se agregó:  0.0 en:  1 11\n",
            "el NOT IN text_03\n",
            "se agregó:  0.0 en:  2 11\n",
            "metro IN text_01\n",
            "se agregó:  1.0 en:  0 12\n",
            "metro NOT IN text_02\n",
            "se agregó:  0.0 en:  1 12\n",
            "metro NOT IN text_03\n",
            "se agregó:  0.0 en:  2 12\n",
            "verdad IN text_01\n",
            "se agregó:  1.0 en:  0 13\n",
            "verdad NOT IN text_02\n",
            "se agregó:  0.0 en:  1 13\n",
            "verdad NOT IN text_03\n",
            "se agregó:  0.0 en:  2 13\n",
            "acabo IN text_01\n",
            "se agregó:  1.0 en:  0 14\n",
            "acabo NOT IN text_02\n",
            "se agregó:  0.0 en:  1 14\n",
            "acabo NOT IN text_03\n",
            "se agregó:  0.0 en:  2 14\n",
            "ver IN text_01\n",
            "se agregó:  1.0 en:  0 15\n",
            "ver NOT IN text_02\n",
            "se agregó:  0.0 en:  1 15\n",
            "ver NOT IN text_03\n",
            "se agregó:  0.0 en:  2 15\n",
            "cuatro IN text_01\n",
            "se agregó:  1.0 en:  0 16\n",
            "cuatro NOT IN text_02\n",
            "se agregó:  0.0 en:  1 16\n",
            "cuatro NOT IN text_03\n",
            "se agregó:  0.0 en:  2 16\n",
            "parejas IN text_01\n",
            "se agregó:  1.0 en:  0 17\n",
            "parejas NOT IN text_02\n",
            "se agregó:  0.0 en:  1 17\n",
            "parejas NOT IN text_03\n",
            "se agregó:  0.0 en:  2 17\n",
            "la IN text_01\n",
            "se agregó:  1.0 en:  0 18\n",
            "la IN text_02\n",
            "se agregó:  1.0 en:  1 18\n",
            "la NOT IN text_03\n",
            "se agregó:  0.0 en:  2 18\n",
            "misma IN text_01\n",
            "se agregó:  1.0 en:  0 19\n",
            "misma NOT IN text_02\n",
            "se agregó:  0.0 en:  1 19\n",
            "misma NOT IN text_03\n",
            "se agregó:  0.0 en:  2 19\n",
            "área IN text_01\n",
            "se agregó:  1.0 en:  0 20\n",
            "área NOT IN text_02\n",
            "se agregó:  0.0 en:  1 20\n",
            "área NOT IN text_03\n",
            "se agregó:  0.0 en:  2 20\n",
            "dandolo IN text_01\n",
            "se agregó:  1.0 en:  0 21\n",
            "dandolo NOT IN text_02\n",
            "se agregó:  0.0 en:  1 21\n",
            "dandolo NOT IN text_03\n",
            "se agregó:  0.0 en:  2 21\n",
            "todo IN text_01\n",
            "se agregó:  1.0 en:  0 22\n",
            "todo NOT IN text_02\n",
            "se agregó:  0.0 en:  1 22\n",
            "todo NOT IN text_03\n",
            "se agregó:  0.0 en:  2 22\n",
            "con IN text_01\n",
            "se agregó:  1.0 en:  0 23\n",
            "con NOT IN text_02\n",
            "se agregó:  0.0 en:  1 23\n",
            "con NOT IN text_03\n",
            "se agregó:  0.0 en:  2 23\n",
            "su IN text_01\n",
            "se agregó:  1.0 en:  0 24\n",
            "su NOT IN text_02\n",
            "se agregó:  0.0 en:  1 24\n",
            "su NOT IN text_03\n",
            "se agregó:  0.0 en:  2 24\n",
            "queso IN text_01\n",
            "se agregó:  1.0 en:  0 25\n",
            "queso NOT IN text_02\n",
            "se agregó:  0.0 en:  1 25\n",
            "queso NOT IN text_03\n",
            "se agregó:  0.0 en:  2 25\n",
            "dos NOT IN text_01\n",
            "se agregó:  0.0 en:  0 26\n",
            "dos IN text_02\n",
            "se agregó:  1.0 en:  1 26\n",
            "dos NOT IN text_03\n",
            "se agregó:  0.0 en:  2 26\n",
            "mujeres NOT IN text_01\n",
            "se agregó:  0.0 en:  0 27\n",
            "mujeres IN text_02\n",
            "se agregó:  1.0 en:  1 27\n",
            "mujeres NOT IN text_03\n",
            "se agregó:  0.0 en:  2 27\n",
            "se NOT IN text_01\n",
            "se agregó:  0.0 en:  0 28\n",
            "se IN text_02\n",
            "se agregó:  1.0 en:  1 28\n",
            "se NOT IN text_03\n",
            "se agregó:  0.0 en:  2 28\n",
            "tomaban NOT IN text_01\n",
            "se agregó:  0.0 en:  0 29\n",
            "tomaban IN text_02\n",
            "se agregó:  1.0 en:  1 29\n",
            "tomaban NOT IN text_03\n",
            "se agregó:  0.0 en:  2 29\n",
            "una NOT IN text_01\n",
            "se agregó:  0.0 en:  0 30\n",
            "una IN text_02\n",
            "se agregó:  1.0 en:  1 30\n",
            "una NOT IN text_03\n",
            "se agregó:  0.0 en:  2 30\n",
            "selfie NOT IN text_01\n",
            "se agregó:  0.0 en:  0 31\n",
            "selfie IN text_02\n",
            "se agregó:  1.0 en:  1 31\n",
            "selfie NOT IN text_03\n",
            "se agregó:  0.0 en:  2 31\n",
            "al NOT IN text_01\n",
            "se agregó:  0.0 en:  0 32\n",
            "al IN text_02\n",
            "se agregó:  1.0 en:  1 32\n",
            "al NOT IN text_03\n",
            "se agregó:  0.0 en:  2 32\n",
            "lado NOT IN text_01\n",
            "se agregó:  0.0 en:  0 33\n",
            "lado IN text_02\n",
            "se agregó:  1.0 en:  1 33\n",
            "lado NOT IN text_03\n",
            "se agregó:  0.0 en:  2 33\n",
            "pista NOT IN text_01\n",
            "se agregó:  0.0 en:  0 34\n",
            "pista IN text_02\n",
            "se agregó:  1.0 en:  1 34\n",
            "pista NOT IN text_03\n",
            "se agregó:  0.0 en:  2 34\n",
            "aterrizaje NOT IN text_01\n",
            "se agregó:  0.0 en:  0 35\n",
            "aterrizaje IN text_02\n",
            "se agregó:  1.0 en:  1 35\n",
            "aterrizaje NOT IN text_03\n",
            "se agregó:  0.0 en:  2 35\n",
            "serrana NOT IN text_01\n",
            "se agregó:  0.0 en:  0 36\n",
            "serrana IN text_02\n",
            "se agregó:  1.0 en:  1 36\n",
            "serrana NOT IN text_03\n",
            "se agregó:  0.0 en:  2 36\n",
            "murieron NOT IN text_01\n",
            "se agregó:  0.0 en:  0 37\n",
            "murieron IN text_02\n",
            "se agregó:  1.0 en:  1 37\n",
            "murieron NOT IN text_03\n",
            "se agregó:  0.0 en:  2 37\n",
            "parecer NOT IN text_01\n",
            "se agregó:  0.0 en:  0 38\n",
            "parecer IN text_02\n",
            "se agregó:  1.0 en:  1 38\n",
            "parecer NOT IN text_03\n",
            "se agregó:  0.0 en:  2 38\n",
            "accidentalmente NOT IN text_01\n",
            "se agregó:  0.0 en:  0 39\n",
            "accidentalmente IN text_02\n",
            "se agregó:  1.0 en:  1 39\n",
            "accidentalmente NOT IN text_03\n",
            "se agregó:  0.0 en:  2 39\n",
            "un NOT IN text_01\n",
            "se agregó:  0.0 en:  0 40\n",
            "un IN text_02\n",
            "se agregó:  1.0 en:  1 40\n",
            "un NOT IN text_03\n",
            "se agregó:  0.0 en:  2 40\n",
            "piloto NOT IN text_01\n",
            "se agregó:  0.0 en:  0 41\n",
            "piloto IN text_02\n",
            "se agregó:  1.0 en:  1 41\n",
            "piloto NOT IN text_03\n",
            "se agregó:  0.0 en:  2 41\n",
            "aviador NOT IN text_01\n",
            "se agregó:  0.0 en:  0 42\n",
            "aviador IN text_02\n",
            "se agregó:  1.0 en:  1 42\n",
            "aviador NOT IN text_03\n",
            "se agregó:  0.0 en:  2 42\n",
            "solía NOT IN text_01\n",
            "se agregó:  0.0 en:  0 43\n",
            "solía IN text_02\n",
            "se agregó:  1.0 en:  1 43\n",
            "solía NOT IN text_03\n",
            "se agregó:  0.0 en:  2 43\n",
            "viajar NOT IN text_01\n",
            "se agregó:  0.0 en:  0 44\n",
            "viajar IN text_02\n",
            "se agregó:  1.0 en:  1 44\n",
            "viajar NOT IN text_03\n",
            "se agregó:  0.0 en:  2 44\n",
            "sierra NOT IN text_01\n",
            "se agregó:  0.0 en:  0 45\n",
            "sierra IN text_02\n",
            "se agregó:  1.0 en:  1 45\n",
            "sierra NOT IN text_03\n",
            "se agregó:  0.0 en:  2 45\n",
            "tarahumara NOT IN text_01\n",
            "se agregó:  0.0 en:  0 46\n",
            "tarahumara IN text_02\n",
            "se agregó:  1.0 en:  1 46\n",
            "tarahumara NOT IN text_03\n",
            "se agregó:  0.0 en:  2 46\n",
            "maestro NOT IN text_01\n",
            "se agregó:  0.0 en:  0 47\n",
            "maestro IN text_02\n",
            "se agregó:  1.0 en:  1 47\n",
            "maestro NOT IN text_03\n",
            "se agregó:  0.0 en:  2 47\n",
            "artes NOT IN text_01\n",
            "se agregó:  0.0 en:  0 48\n",
            "artes IN text_02\n",
            "se agregó:  1.0 en:  1 48\n",
            "artes NOT IN text_03\n",
            "se agregó:  0.0 en:  2 48\n",
            "marciales NOT IN text_01\n",
            "se agregó:  0.0 en:  0 49\n",
            "marciales IN text_02\n",
            "se agregó:  1.0 en:  1 49\n",
            "marciales NOT IN text_03\n",
            "se agregó:  0.0 en:  2 49\n",
            "retirado NOT IN text_01\n",
            "se agregó:  0.0 en:  0 50\n",
            "retirado IN text_02\n",
            "se agregó:  1.0 en:  1 50\n",
            "retirado NOT IN text_03\n",
            "se agregó:  0.0 en:  2 50\n",
            "fueron NOT IN text_01\n",
            "se agregó:  0.0 en:  0 51\n",
            "fueron IN text_02\n",
            "se agregó:  1.0 en:  1 51\n",
            "fueron NOT IN text_03\n",
            "se agregó:  0.0 en:  2 51\n",
            "asesinados NOT IN text_01\n",
            "se agregó:  0.0 en:  0 52\n",
            "asesinados IN text_02\n",
            "se agregó:  1.0 en:  1 52\n",
            "asesinados NOT IN text_03\n",
            "se agregó:  0.0 en:  2 52\n",
            "2017 NOT IN text_01\n",
            "se agregó:  0.0 en:  0 53\n",
            "2017 IN text_02\n",
            "se agregó:  1.0 en:  1 53\n",
            "2017 NOT IN text_03\n",
            "se agregó:  0.0 en:  2 53\n",
            "buenos NOT IN text_01\n",
            "se agregó:  0.0 en:  0 54\n",
            "buenos NOT IN text_02\n",
            "se agregó:  0.0 en:  1 54\n",
            "buenos IN text_03\n",
            "se agregó:  1.0 en:  2 54\n",
            "días NOT IN text_01\n",
            "se agregó:  0.0 en:  0 55\n",
            "días NOT IN text_02\n",
            "se agregó:  0.0 en:  1 55\n",
            "días IN text_03\n",
            "se agregó:  1.0 en:  2 55\n",
            "país NOT IN text_01\n",
            "se agregó:  0.0 en:  0 56\n",
            "país NOT IN text_02\n",
            "se agregó:  0.0 en:  1 56\n",
            "país IN text_03\n",
            "se agregó:  1.0 en:  2 56\n",
            "subsecretario NOT IN text_01\n",
            "se agregó:  0.0 en:  0 57\n",
            "subsecretario NOT IN text_02\n",
            "se agregó:  0.0 en:  1 57\n",
            "subsecretario IN text_03\n",
            "se agregó:  1.0 en:  2 57\n",
            "educación NOT IN text_01\n",
            "se agregó:  0.0 en:  0 58\n",
            "educación NOT IN text_02\n",
            "se agregó:  0.0 en:  1 58\n",
            "educación IN text_03\n",
            "se agregó:  1.0 en:  2 58\n",
            "dice NOT IN text_01\n",
            "se agregó:  0.0 en:  0 59\n",
            "dice NOT IN text_02\n",
            "se agregó:  0.0 en:  1 59\n",
            "dice IN text_03\n",
            "se agregó:  1.0 en:  2 59\n",
            "comunismo NOT IN text_01\n",
            "se agregó:  0.0 en:  0 60\n",
            "comunismo NOT IN text_02\n",
            "se agregó:  0.0 en:  1 60\n",
            "comunismo IN text_03\n",
            "se agregó:  1.0 en:  2 60\n",
            "necesario NOT IN text_01\n",
            "se agregó:  0.0 en:  0 61\n",
            "necesario NOT IN text_02\n",
            "se agregó:  0.0 en:  1 61\n",
            "necesario IN text_03\n",
            "se agregó:  1.0 en:  2 61\n",
            "transformar NOT IN text_01\n",
            "se agregó:  0.0 en:  0 62\n",
            "transformar NOT IN text_02\n",
            "se agregó:  0.0 en:  1 62\n",
            "transformar IN text_03\n",
            "se agregó:  1.0 en:  2 62\n",
            "méxico NOT IN text_01\n",
            "se agregó:  0.0 en:  0 63\n",
            "méxico NOT IN text_02\n",
            "se agregó:  0.0 en:  1 63\n",
            "méxico IN text_03\n",
            "se agregó:  1.0 en:  2 63\n",
            "disparan NOT IN text_01\n",
            "se agregó:  0.0 en:  0 64\n",
            "disparan NOT IN text_02\n",
            "se agregó:  0.0 en:  1 64\n",
            "disparan IN text_03\n",
            "se agregó:  1.0 en:  2 64\n",
            "312 NOT IN text_01\n",
            "se agregó:  0.0 en:  0 65\n",
            "312 NOT IN text_02\n",
            "se agregó:  0.0 en:  1 65\n",
            "312 IN text_03\n",
            "se agregó:  1.0 en:  2 65\n",
            "casos NOT IN text_01\n",
            "se agregó:  0.0 en:  0 66\n",
            "casos NOT IN text_02\n",
            "se agregó:  0.0 en:  1 66\n",
            "casos IN text_03\n",
            "se agregó:  1.0 en:  2 66\n",
            "dengue NOT IN text_01\n",
            "se agregó:  0.0 en:  0 67\n",
            "dengue NOT IN text_02\n",
            "se agregó:  0.0 en:  1 67\n",
            "dengue IN text_03\n",
            "se agregó:  1.0 en:  2 67\n",
            "compraron NOT IN text_01\n",
            "se agregó:  0.0 en:  0 68\n",
            "compraron NOT IN text_02\n",
            "se agregó:  0.0 en:  1 68\n",
            "compraron IN text_03\n",
            "se agregó:  1.0 en:  2 68\n",
            "insecticidas NOT IN text_01\n",
            "se agregó:  0.0 en:  0 69\n",
            "insecticidas NOT IN text_02\n",
            "se agregó:  0.0 en:  1 69\n",
            "insecticidas IN text_03\n",
            "se agregó:  1.0 en:  2 69\n",
            "mireles NOT IN text_01\n",
            "se agregó:  0.0 en:  0 70\n",
            "mireles NOT IN text_02\n",
            "se agregó:  0.0 en:  1 70\n",
            "mireles IN text_03\n",
            "se agregó:  1.0 en:  2 70\n",
            "llama NOT IN text_01\n",
            "se agregó:  0.0 en:  0 71\n",
            "llama NOT IN text_02\n",
            "se agregó:  0.0 en:  1 71\n",
            "llama IN text_03\n",
            "se agregó:  1.0 en:  2 71\n",
            "pirujas NOT IN text_01\n",
            "se agregó:  0.0 en:  0 72\n",
            "pirujas NOT IN text_02\n",
            "se agregó:  0.0 en:  1 72\n",
            "pirujas IN text_03\n",
            "se agregó:  1.0 en:  2 72\n",
            "concubinas NOT IN text_01\n",
            "se agregó:  0.0 en:  0 73\n",
            "concubinas NOT IN text_02\n",
            "se agregó:  0.0 en:  1 73\n",
            "concubinas IN text_03\n",
            "se agregó:  1.0 en:  2 73\n",
            "presi NOT IN text_01\n",
            "se agregó:  0.0 en:  0 74\n",
            "presi NOT IN text_02\n",
            "se agregó:  0.0 en:  1 74\n",
            "presi IN text_03\n",
            "se agregó:  1.0 en:  2 74\n",
            "quiere NOT IN text_01\n",
            "se agregó:  0.0 en:  0 75\n",
            "quiere NOT IN text_02\n",
            "se agregó:  0.0 en:  1 75\n",
            "quiere IN text_03\n",
            "se agregó:  1.0 en:  2 75\n",
            "quitar NOT IN text_01\n",
            "se agregó:  0.0 en:  0 76\n",
            "quitar NOT IN text_02\n",
            "se agregó:  0.0 en:  1 76\n",
            "quitar IN text_03\n",
            "se agregó:  1.0 en:  2 76\n",
            "ine NOT IN text_01\n",
            "se agregó:  0.0 en:  0 77\n",
            "ine NOT IN text_02\n",
            "se agregó:  0.0 en:  1 77\n",
            "ine IN text_03\n",
            "se agregó:  1.0 en:  2 77\n",
            "café NOT IN text_01\n",
            "se agregó:  0.0 en:  0 78\n",
            "café NOT IN text_02\n",
            "se agregó:  0.0 en:  1 78\n",
            "café IN text_03\n",
            "se agregó:  1.0 en:  2 78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w04Hvv0UkJ7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6be2ca05-5027-437f-a27e-86458f21bad1"
      },
      "source": [
        "matrix"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IfJt0FpkJ9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f5a802b-04cf-4aec-c02c-89cc2b6df873"
      },
      "source": [
        "matrix.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 79)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojnw04K0k7bc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3b8fc067-57a5-4b51-de65-7ba4899a3c21"
      },
      "source": [
        "matrix[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6weUVhek_pl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49b75573-626c-4e59-f0b3-61c78393c00f"
      },
      "source": [
        "bin_cos_t01_t02 = dot(matrix[0],matrix[1])/(norm(matrix[0])*norm(matrix[1]))\n",
        "bin_cos_t01_t02"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1386750490563073"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQe7IQdblQha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c530aafb-eb8f-4641-cf0f-e52cce727fc9"
      },
      "source": [
        "bin_cos_t02_t03 = dot(matrix[1],matrix[2])/(norm(matrix[1])*norm(matrix[2]))\n",
        "bin_cos_t02_t03"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNud9KiMlTc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0cd518b-e1f2-4329-90a9-d8947074c75c"
      },
      "source": [
        "bin_cos_t01_t03 = dot(matrix[0],matrix[2])/(norm(matrix[0])*norm(matrix[2]))\n",
        "bin_cos_t01_t03"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkHX95NZliah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}